{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRRPAS8t1DXJ",
        "outputId": "b7433dab-541b-490b-92ce-09d1f8f90577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ik6bg9pr\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ik6bg9pr\n",
            "  Resolved https://github.com/openai/whisper.git to commit e8622f9afc4eba139bf796c210f5c01081000472\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20230314) (10.1.0)\n",
            "Collecting tiktoken==0.3.3 (from openai-whisper==20230314)\n",
            "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.3->openai-whisper==20230314) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.12.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.6)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20230314) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.3->openai-whisper==20230314) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=798395 sha256=95440a355d003d8699582fefc18af7fa50b1349a88cfc4be7788f41ca0a80e98\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5u2uqdzk/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20230314 tiktoken-0.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "id": "-YsDcsci1xRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install --force-reinstall https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd49pnlFqz-K",
        "outputId": "04ba7eb6-1754-46aa-fe24-031395840e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz\n",
            "  Using cached https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz (2.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mutagen (from yt-dlp==2023.7.6)\n",
            "  Using cached mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
            "Collecting pycryptodomex (from yt-dlp==2023.7.6)\n",
            "  Using cached pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "Collecting websockets (from yt-dlp==2023.7.6)\n",
            "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "Collecting certifi (from yt-dlp==2023.7.6)\n",
            "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "Collecting brotli (from yt-dlp==2023.7.6)\n",
            "  Using cached Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "Building wheels for collected packages: yt-dlp\n",
            "  Building wheel for yt-dlp (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yt-dlp: filename=yt_dlp-2023.7.6-py2.py3-none-any.whl size=2799662 sha256=0da6c06d073b19d3a5cf4b2f0e90597ecc193145c071625067e64c615b562ddd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bb7t3ew7/wheels/4c/91/d1/c5369304e2f7afb660bb6eee093af5a7d3c0ea05a3c1e8c797\n",
            "Successfully built yt-dlp\n",
            "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, certifi, yt-dlp\n",
            "  Attempting uninstall: brotli\n",
            "    Found existing installation: Brotli 1.0.9\n",
            "    Uninstalling Brotli-1.0.9:\n",
            "      Successfully uninstalled Brotli-1.0.9\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 11.0.3\n",
            "    Uninstalling websockets-11.0.3:\n",
            "      Successfully uninstalled websockets-11.0.3\n",
            "  Attempting uninstall: pycryptodomex\n",
            "    Found existing installation: pycryptodomex 3.18.0\n",
            "    Uninstalling pycryptodomex-3.18.0:\n",
            "      Successfully uninstalled pycryptodomex-3.18.0\n",
            "  Attempting uninstall: mutagen\n",
            "    Found existing installation: mutagen 1.46.0\n",
            "    Uninstalling mutagen-1.46.0:\n",
            "      Successfully uninstalled mutagen-1.46.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2023.7.22\n",
            "    Uninstalling certifi-2023.7.22:\n",
            "      Successfully uninstalled certifi-2023.7.22\n",
            "  Attempting uninstall: yt-dlp\n",
            "    Found existing installation: yt-dlp 2023.7.6\n",
            "    Uninstalling yt-dlp-2023.7.6:\n",
            "      Successfully uninstalled yt-dlp-2023.7.6\n",
            "Successfully installed brotli-1.0.9 certifi-2023.7.22 mutagen-1.46.0 pycryptodomex-3.18.0 websockets-11.0.3 yt-dlp-2023.7.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yt_dlp as youtube_dl\n",
        "import whisper"
      ],
      "metadata": {
        "id": "C-EKH1p3q6dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yt-dlp -f 'ba' https://www.youtube.com/watch?v=0aqiPyTJv8E -o '%(title)s.%(ext)s'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeYk0kwTrFD9",
        "outputId": "355d9543-2576-42ad-8f38-44caeb004f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=0aqiPyTJv8E\n",
            "[youtube] 0aqiPyTJv8E: Downloading webpage\n",
            "[youtube] 0aqiPyTJv8E: Downloading ios player API JSON\n",
            "[youtube] 0aqiPyTJv8E: Downloading android player API JSON\n",
            "[youtube] 0aqiPyTJv8E: Downloading m3u8 information\n",
            "[info] 0aqiPyTJv8E: Downloading 1 format(s): 251\n",
            "[download] Destination: Is this safest place in the world？ BBC News.webm\n",
            "\u001b[K[download] 100% of    2.26MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m17.27MiB/s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"/content/Safest_Place_in_World.webm\" --model medium --language en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1pP466t8kpv",
        "outputId": "8eb6ea16-817c-4a73-a2a8-1b0c56d00917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:13<00:00, 116MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "[00:00.000 --> 00:05.000]  In the punishing cold of an arctic mountain in the remote Svalbard Islands,\n",
            "[00:05.000 --> 00:09.000]  a doorway leads to what's meant to be the safest place on Earth.\n",
            "[00:09.000 --> 00:12.000]  Scientists are on their way,\n",
            "[00:12.000 --> 00:15.000]  approaching through this isolated and hostile terrain.\n",
            "[00:15.000 --> 00:19.000]  And I'm with them, as they carry a precious cargo of seeds\n",
            "[00:19.000 --> 00:23.000]  to be kept out of the way of whatever climate change might bring.\n",
            "[00:25.000 --> 00:27.000]  How often do you get these deliveries?\n",
            "[00:27.000 --> 00:29.000]  We have deliveries three times a year.\n",
            "[00:29.000 --> 00:34.000]  The box of seeds is about to go through the first line of security.\n",
            "[00:34.000 --> 00:36.000]  There are half a dozen in all.\n",
            "[00:36.000 --> 00:41.000]  I've just come down the access tunnel that's cut into the mountain here.\n",
            "[00:41.000 --> 00:44.000]  This place is 130 metres above sea level,\n",
            "[00:44.000 --> 00:49.000]  because if the worst happens and global warming melts all of the polar ice caps,\n",
            "[00:49.000 --> 00:52.000]  this project will still be safe.\n",
            "[00:54.000 --> 00:56.000]  OK, so this is the next stage.\n",
            "[00:56.000 --> 01:00.000]  The deeper inside the mountain we go, the more the temperature drops.\n",
            "[01:00.000 --> 01:03.000]  The store is designed to survive any natural disaster.\n",
            "[01:03.000 --> 01:06.000]  The seeds can last here for a very long time.\n",
            "[01:06.000 --> 01:08.000]  It depends on what crop it is,\n",
            "[01:08.000 --> 01:11.000]  but some of the crops may survive for more than 4,000 years.\n",
            "[01:11.000 --> 01:14.000]  You're really imagining this place functioning,\n",
            "[01:14.000 --> 01:17.000]  keeping the seeds safe for 4,000 years?\n",
            "[01:17.000 --> 01:19.000]  Well, it's difficult to say.\n",
            "[01:19.000 --> 01:23.000]  I'm sure that the pharaohs thought that their pyramids would last long, and they did.\n",
            "[01:26.000 --> 01:29.000]  The last barrier to the store itself.\n",
            "[01:29.000 --> 01:30.000]  OK, here we go.\n",
            "[01:30.000 --> 01:34.000]  Inside here, it's minus 18 Celsius.\n",
            "[01:34.000 --> 01:38.000]  The rows of shelves are filling up with seeds from all over the world.\n",
            "[01:38.000 --> 01:42.000]  There are samples of nearly half of the most important food crops,\n",
            "[01:42.000 --> 01:45.000]  brought here just in case.\n",
            "[01:45.000 --> 01:49.000]  Samples of seeds used to be held in glass test tubes.\n",
            "[01:49.000 --> 01:52.000]  Now they're kept in little plastic packets,\n",
            "[01:52.000 --> 01:56.000]  and there are more than 800,000 of these in this vault.\n",
            "[01:56.000 --> 02:00.000]  And everywhere you look, there are examples of why this place matters.\n",
            "[02:00.000 --> 02:04.000]  There are seeds from Syria, plants that are good at coping with drought,\n",
            "[02:04.000 --> 02:07.000]  and some have just been returned to the Middle East.\n",
            "[02:07.000 --> 02:10.000]  When harvests are ruined by extremes of weather,\n",
            "[02:10.000 --> 02:14.000]  having backup copies of key seeds is essential.\n",
            "[02:14.000 --> 02:19.000]  Another threat is flooding, which can damage national stores of seeds.\n",
            "[02:19.000 --> 02:22.000]  This happened in the Philippines.\n",
            "[02:22.000 --> 02:28.000]  And with industrial-scale farming, most food comes from just a dozen varieties of plants.\n",
            "[02:28.000 --> 02:32.000]  So keeping different genetic types helps to guarantee supplies.\n",
            "[02:32.000 --> 02:35.000]  It is for the survival of mankind in the future.\n",
            "[02:35.000 --> 02:40.000]  I mean, we need the diversity, all the different kinds of plants' material,\n",
            "[02:40.000 --> 02:42.000]  to get food for the future.\n",
            "[02:42.000 --> 02:45.000]  We have a lot of problems seeing us now.\n",
            "[02:45.000 --> 02:51.000]  Climate change, environmental problems, and to tackle that, we need genetic variation.\n",
            "[02:51.000 --> 02:57.000]  So in these remote mountains, this place is meant to be a safeguard against apocalypse,\n",
            "[02:57.000 --> 03:00.000]  an insurance policy for a warming world.\n",
            "[03:00.000 --> 03:05.000]  David Trucman, BBC News, in Svalbard, in the Arctic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"large\")\n",
        "result = model.transcribe(\"/content/Safest_Place_in_World.webm\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "infhHRMD5MYc",
        "outputId": "87871c46-6e87-4e0d-e73e-b59628feef5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:30<00:00, 101MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In the punishing cold of an arctic mountain in the remote Svalbard Islands, a doorway leads to what's meant to be the safest place on earth. Scientists are on their way, approaching through this isolated and hostile terrain. And I'm with them, as they carry a precious cargo of seeds, to be kept out of the way of whatever climate change might bring. How often do you get these deliveries? We have deliveries three times a year. The box of seeds is about to go through the first line of security. There are half a dozen in all. I've just come down the access tunnel that's cut into the mountain here. This place is 130 metres above sea level, because if the worst happens and global warming melts all of the polar ice caps, this project will still be safe. OK, so this is the next stage. The deeper inside the mountain we go, the more the temperature drops. The store is designed to survive any natural disaster. The seeds can last here for a very long time. It depends on what crop it is, but some of the crops may survive for more than 4,000 years. You're really imagining this place functioning, keeping the seeds safe for 4,000 years? Well, it's difficult to say. I'm sure that the pharaohs thought that their pyramids would last long, and they did. The last barrier to the store itself. OK, here we go. Inside here, it's minus 18 Celsius. The rows of shelves are filling up with seeds from all over the world. There are samples of nearly half of the most important food crops, brought here just in case. Samples of seeds used to be held in glass test tubes. Now they're kept in little plastic packets, and there are 800,000 of these in this vault. And everywhere you look, there are examples of why this place matters. There are seeds from Syria, plants that are good at coping with drought, and some have just been returned to the Middle East. When harvests are ruined by extremes of weather, having backup copies of key seeds is essential. Another threat is flooding, which can damage national stores of seeds. This happened in the Philippines. And with industrial-scale farming, most food comes from just a dozen varieties of plants. So keeping different genetic types helps to guarantee supplies. It is for the survival of mankind in the future. We need the diversity, all the different kinds of plant material, to get food for the future. We have a lot of problems seeing us now. Climate change, environmental problems, and to tackle that, we need genetic variation. So in these remote mountains, this place is meant to be a safeguard against apocalypse, an insurance policy for a warming world. David Shukman, BBC News, in Svalbard in the Arctic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrLqKUmXs5LB",
        "outputId": "37ffebcf-09e3-486f-844b-3e7e6c54cfd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \" In the punishing cold of an arctic mountain in the remote Svalbard Islands, a doorway leads to what's meant to be the safest place on earth. Scientists are on their way, approaching through this isolated and hostile terrain. And I'm with them, as they carry a precious cargo of seeds, to be kept out of the way of whatever climate change might bring. How often do you get these deliveries? We have deliveries three times a year. The box of seeds is about to go through the first line of security. There are half a dozen in all. I've just come down the access tunnel that's cut into the mountain here. This place is 130 metres above sea level, because if the worst happens and global warming melts all of the polar ice caps, this project will still be safe. OK, so this is the next stage. The deeper inside the mountain we go, the more the temperature drops. The store is designed to survive any natural disaster. The seeds can last here for a very long time. It depends on what crop it is, but some of the crops may survive for more than 4,000 years. You're really imagining this place functioning, keeping the seeds safe for 4,000 years? Well, it's difficult to say. I'm sure that the pharaohs thought that their pyramids would last long, and they did. The last barrier to the store itself. OK, here we go. Inside here, it's minus 18 Celsius. The rows of shelves are filling up with seeds from all over the world. There are samples of nearly half of the most important food crops, brought here just in case. Samples of seeds used to be held in glass test tubes. Now they're kept in little plastic packets, and there are 800,000 of these in this vault. And everywhere you look, there are examples of why this place matters. There are seeds from Syria, plants that are good at coping with drought, and some have just been returned to the Middle East. When harvests are ruined by extremes of weather, having backup copies of key seeds is essential. Another threat is flooding, which can damage national stores of seeds. This happened in the Philippines. And with industrial-scale farming, most food comes from just a dozen varieties of plants. So keeping different genetic types helps to guarantee supplies. It is for the survival of mankind in the future. We need the diversity, all the different kinds of plant material, to get food for the future. We have a lot of problems seeing us now. Climate change, environmental problems, and to tackle that, we need genetic variation. So in these remote mountains, this place is meant to be a safeguard against apocalypse, an insurance policy for a warming world. David Shukman, BBC News, in Svalbard in the Arctic.\", 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 5.0, 'text': ' In the punishing cold of an arctic mountain in the remote Svalbard Islands,', 'tokens': [50364, 682, 264, 49824, 3554, 295, 364, 594, 15518, 6937, 294, 264, 8607, 318, 3337, 29984, 23492, 11, 50614], 'temperature': 0.0, 'avg_logprob': -0.2446273456920277, 'compression_ratio': 1.556, 'no_speech_prob': 0.43455615639686584}, {'id': 1, 'seek': 0, 'start': 5.0, 'end': 9.0, 'text': \" a doorway leads to what's meant to be the safest place on earth.\", 'tokens': [50614, 257, 41992, 6689, 281, 437, 311, 4140, 281, 312, 264, 37558, 1081, 322, 4120, 13, 50814], 'temperature': 0.0, 'avg_logprob': -0.2446273456920277, 'compression_ratio': 1.556, 'no_speech_prob': 0.43455615639686584}, {'id': 2, 'seek': 0, 'start': 9.0, 'end': 12.0, 'text': ' Scientists are on their way,', 'tokens': [50814, 32958, 366, 322, 641, 636, 11, 50964], 'temperature': 0.0, 'avg_logprob': -0.2446273456920277, 'compression_ratio': 1.556, 'no_speech_prob': 0.43455615639686584}, {'id': 3, 'seek': 0, 'start': 12.0, 'end': 15.0, 'text': ' approaching through this isolated and hostile terrain.', 'tokens': [50964, 14908, 807, 341, 14621, 293, 27312, 17674, 13, 51114], 'temperature': 0.0, 'avg_logprob': -0.2446273456920277, 'compression_ratio': 1.556, 'no_speech_prob': 0.43455615639686584}, {'id': 4, 'seek': 0, 'start': 15.0, 'end': 19.0, 'text': \" And I'm with them, as they carry a precious cargo of seeds,\", 'tokens': [51114, 400, 286, 478, 365, 552, 11, 382, 436, 3985, 257, 12406, 19449, 295, 9203, 11, 51314], 'temperature': 0.0, 'avg_logprob': -0.2446273456920277, 'compression_ratio': 1.556, 'no_speech_prob': 0.43455615639686584}, {'id': 5, 'seek': 0, 'start': 19.0, 'end': 23.0, 'text': ' to be kept out of the way of whatever climate change might bring.', 'tokens': [51314, 281, 312, 4305, 484, 295, 264, 636, 295, 2035, 5659, 1319, 1062, 1565, 13, 51514], 'temperature': 0.0, 'avg_logprob': -0.2446273456920277, 'compression_ratio': 1.556, 'no_speech_prob': 0.43455615639686584}, {'id': 6, 'seek': 0, 'start': 25.0, 'end': 27.0, 'text': ' How often do you get these deliveries?', 'tokens': [51614, 1012, 2049, 360, 291, 483, 613, 46448, 30, 51714], 'temperature': 0.0, 'avg_logprob': -0.2446273456920277, 'compression_ratio': 1.556, 'no_speech_prob': 0.43455615639686584}, {'id': 7, 'seek': 2700, 'start': 27.0, 'end': 29.0, 'text': ' We have deliveries three times a year.', 'tokens': [50364, 492, 362, 46448, 1045, 1413, 257, 1064, 13, 50464], 'temperature': 0.0, 'avg_logprob': -0.19661208818543632, 'compression_ratio': 1.55078125, 'no_speech_prob': 0.011990648694336414}, {'id': 8, 'seek': 2700, 'start': 29.0, 'end': 34.0, 'text': ' The box of seeds is about to go through the first line of security.', 'tokens': [50464, 440, 2424, 295, 9203, 307, 466, 281, 352, 807, 264, 700, 1622, 295, 3825, 13, 50714], 'temperature': 0.0, 'avg_logprob': -0.19661208818543632, 'compression_ratio': 1.55078125, 'no_speech_prob': 0.011990648694336414}, {'id': 9, 'seek': 2700, 'start': 34.0, 'end': 36.0, 'text': ' There are half a dozen in all.', 'tokens': [50714, 821, 366, 1922, 257, 16654, 294, 439, 13, 50814], 'temperature': 0.0, 'avg_logprob': -0.19661208818543632, 'compression_ratio': 1.55078125, 'no_speech_prob': 0.011990648694336414}, {'id': 10, 'seek': 2700, 'start': 36.0, 'end': 41.0, 'text': \" I've just come down the access tunnel that's cut into the mountain here.\", 'tokens': [50814, 286, 600, 445, 808, 760, 264, 2105, 13186, 300, 311, 1723, 666, 264, 6937, 510, 13, 51064], 'temperature': 0.0, 'avg_logprob': -0.19661208818543632, 'compression_ratio': 1.55078125, 'no_speech_prob': 0.011990648694336414}, {'id': 11, 'seek': 2700, 'start': 41.0, 'end': 45.0, 'text': ' This place is 130 metres above sea level,', 'tokens': [51064, 639, 1081, 307, 19966, 23861, 3673, 4158, 1496, 11, 51264], 'temperature': 0.0, 'avg_logprob': -0.19661208818543632, 'compression_ratio': 1.55078125, 'no_speech_prob': 0.011990648694336414}, {'id': 12, 'seek': 2700, 'start': 45.0, 'end': 49.0, 'text': ' because if the worst happens and global warming melts all of the polar ice caps,', 'tokens': [51264, 570, 498, 264, 5855, 2314, 293, 4338, 17983, 30136, 439, 295, 264, 12367, 4435, 13855, 11, 51464], 'temperature': 0.0, 'avg_logprob': -0.19661208818543632, 'compression_ratio': 1.55078125, 'no_speech_prob': 0.011990648694336414}, {'id': 13, 'seek': 2700, 'start': 49.0, 'end': 52.0, 'text': ' this project will still be safe.', 'tokens': [51464, 341, 1716, 486, 920, 312, 3273, 13, 51614], 'temperature': 0.0, 'avg_logprob': -0.19661208818543632, 'compression_ratio': 1.55078125, 'no_speech_prob': 0.011990648694336414}, {'id': 14, 'seek': 2700, 'start': 54.0, 'end': 56.0, 'text': ' OK, so this is the next stage.', 'tokens': [51714, 2264, 11, 370, 341, 307, 264, 958, 3233, 13, 51814], 'temperature': 0.0, 'avg_logprob': -0.19661208818543632, 'compression_ratio': 1.55078125, 'no_speech_prob': 0.011990648694336414}, {'id': 15, 'seek': 5600, 'start': 56.0, 'end': 60.0, 'text': ' The deeper inside the mountain we go, the more the temperature drops.', 'tokens': [50364, 440, 7731, 1854, 264, 6937, 321, 352, 11, 264, 544, 264, 4292, 11438, 13, 50564], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 16, 'seek': 5600, 'start': 60.0, 'end': 64.0, 'text': ' The store is designed to survive any natural disaster.', 'tokens': [50564, 440, 3531, 307, 4761, 281, 7867, 604, 3303, 11293, 13, 50764], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 17, 'seek': 5600, 'start': 64.0, 'end': 66.0, 'text': ' The seeds can last here for a very long time.', 'tokens': [50764, 440, 9203, 393, 1036, 510, 337, 257, 588, 938, 565, 13, 50864], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 18, 'seek': 5600, 'start': 66.0, 'end': 68.0, 'text': ' It depends on what crop it is,', 'tokens': [50864, 467, 5946, 322, 437, 9086, 309, 307, 11, 50964], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 19, 'seek': 5600, 'start': 68.0, 'end': 72.0, 'text': ' but some of the crops may survive for more than 4,000 years.', 'tokens': [50964, 457, 512, 295, 264, 16829, 815, 7867, 337, 544, 813, 1017, 11, 1360, 924, 13, 51164], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 20, 'seek': 5600, 'start': 72.0, 'end': 75.0, 'text': \" You're really imagining this place functioning,\", 'tokens': [51164, 509, 434, 534, 27798, 341, 1081, 18483, 11, 51314], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 21, 'seek': 5600, 'start': 75.0, 'end': 77.0, 'text': ' keeping the seeds safe for 4,000 years?', 'tokens': [51314, 5145, 264, 9203, 3273, 337, 1017, 11, 1360, 924, 30, 51414], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 22, 'seek': 5600, 'start': 77.0, 'end': 79.0, 'text': \" Well, it's difficult to say.\", 'tokens': [51414, 1042, 11, 309, 311, 2252, 281, 584, 13, 51514], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 23, 'seek': 5600, 'start': 79.0, 'end': 84.0, 'text': \" I'm sure that the pharaohs thought that their pyramids would last long, and they did.\", 'tokens': [51514, 286, 478, 988, 300, 264, 903, 34788, 82, 1194, 300, 641, 20543, 3742, 576, 1036, 938, 11, 293, 436, 630, 13, 51764], 'temperature': 0.0, 'avg_logprob': -0.1615976825837166, 'compression_ratio': 1.6787003610108304, 'no_speech_prob': 0.0028808529023081064}, {'id': 24, 'seek': 8600, 'start': 86.0, 'end': 89.0, 'text': ' The last barrier to the store itself.', 'tokens': [50364, 440, 1036, 13357, 281, 264, 3531, 2564, 13, 50514], 'temperature': 0.0, 'avg_logprob': -0.1760808610424553, 'compression_ratio': 1.4978723404255319, 'no_speech_prob': 0.004484165459871292}, {'id': 25, 'seek': 8600, 'start': 89.0, 'end': 91.0, 'text': ' OK, here we go.', 'tokens': [50514, 2264, 11, 510, 321, 352, 13, 50614], 'temperature': 0.0, 'avg_logprob': -0.1760808610424553, 'compression_ratio': 1.4978723404255319, 'no_speech_prob': 0.004484165459871292}, {'id': 26, 'seek': 8600, 'start': 91.0, 'end': 94.0, 'text': \" Inside here, it's minus 18 Celsius.\", 'tokens': [50614, 15123, 510, 11, 309, 311, 3175, 2443, 22658, 13, 50764], 'temperature': 0.0, 'avg_logprob': -0.1760808610424553, 'compression_ratio': 1.4978723404255319, 'no_speech_prob': 0.004484165459871292}, {'id': 27, 'seek': 8600, 'start': 94.0, 'end': 98.0, 'text': ' The rows of shelves are filling up with seeds from all over the world.', 'tokens': [50764, 440, 13241, 295, 24349, 366, 10623, 493, 365, 9203, 490, 439, 670, 264, 1002, 13, 50964], 'temperature': 0.0, 'avg_logprob': -0.1760808610424553, 'compression_ratio': 1.4978723404255319, 'no_speech_prob': 0.004484165459871292}, {'id': 28, 'seek': 8600, 'start': 98.0, 'end': 102.0, 'text': ' There are samples of nearly half of the most important food crops,', 'tokens': [50964, 821, 366, 10938, 295, 6217, 1922, 295, 264, 881, 1021, 1755, 16829, 11, 51164], 'temperature': 0.0, 'avg_logprob': -0.1760808610424553, 'compression_ratio': 1.4978723404255319, 'no_speech_prob': 0.004484165459871292}, {'id': 29, 'seek': 8600, 'start': 102.0, 'end': 105.0, 'text': ' brought here just in case.', 'tokens': [51164, 3038, 510, 445, 294, 1389, 13, 51314], 'temperature': 0.0, 'avg_logprob': -0.1760808610424553, 'compression_ratio': 1.4978723404255319, 'no_speech_prob': 0.004484165459871292}, {'id': 30, 'seek': 8600, 'start': 105.0, 'end': 110.0, 'text': ' Samples of seeds used to be held in glass test tubes.', 'tokens': [51314, 4832, 2622, 295, 9203, 1143, 281, 312, 5167, 294, 4276, 1500, 21458, 13, 51564], 'temperature': 0.0, 'avg_logprob': -0.1760808610424553, 'compression_ratio': 1.4978723404255319, 'no_speech_prob': 0.004484165459871292}, {'id': 31, 'seek': 8600, 'start': 110.0, 'end': 113.0, 'text': \" Now they're kept in little plastic packets,\", 'tokens': [51564, 823, 436, 434, 4305, 294, 707, 5900, 30364, 11, 51714], 'temperature': 0.0, 'avg_logprob': -0.1760808610424553, 'compression_ratio': 1.4978723404255319, 'no_speech_prob': 0.004484165459871292}, {'id': 32, 'seek': 11300, 'start': 113.0, 'end': 116.0, 'text': ' and there are 800,000 of these in this vault.', 'tokens': [50364, 293, 456, 366, 13083, 11, 1360, 295, 613, 294, 341, 27134, 13, 50514], 'temperature': 0.0, 'avg_logprob': -0.19387703641838983, 'compression_ratio': 1.5971223021582734, 'no_speech_prob': 0.10198593139648438}, {'id': 33, 'seek': 11300, 'start': 116.0, 'end': 120.0, 'text': ' And everywhere you look, there are examples of why this place matters.', 'tokens': [50514, 400, 5315, 291, 574, 11, 456, 366, 5110, 295, 983, 341, 1081, 7001, 13, 50714], 'temperature': 0.0, 'avg_logprob': -0.19387703641838983, 'compression_ratio': 1.5971223021582734, 'no_speech_prob': 0.10198593139648438}, {'id': 34, 'seek': 11300, 'start': 120.0, 'end': 124.0, 'text': ' There are seeds from Syria, plants that are good at coping with drought,', 'tokens': [50714, 821, 366, 9203, 490, 13314, 11, 5972, 300, 366, 665, 412, 32893, 365, 22900, 11, 50914], 'temperature': 0.0, 'avg_logprob': -0.19387703641838983, 'compression_ratio': 1.5971223021582734, 'no_speech_prob': 0.10198593139648438}, {'id': 35, 'seek': 11300, 'start': 124.0, 'end': 127.0, 'text': ' and some have just been returned to the Middle East.', 'tokens': [50914, 293, 512, 362, 445, 668, 8752, 281, 264, 10775, 6747, 13, 51064], 'temperature': 0.0, 'avg_logprob': -0.19387703641838983, 'compression_ratio': 1.5971223021582734, 'no_speech_prob': 0.10198593139648438}, {'id': 36, 'seek': 11300, 'start': 127.0, 'end': 130.0, 'text': ' When harvests are ruined by extremes of weather,', 'tokens': [51064, 1133, 2233, 85, 4409, 366, 17013, 538, 41119, 295, 5503, 11, 51214], 'temperature': 0.0, 'avg_logprob': -0.19387703641838983, 'compression_ratio': 1.5971223021582734, 'no_speech_prob': 0.10198593139648438}, {'id': 37, 'seek': 11300, 'start': 130.0, 'end': 133.0, 'text': ' having backup copies of key seeds is essential.', 'tokens': [51214, 1419, 14807, 14341, 295, 2141, 9203, 307, 7115, 13, 51364], 'temperature': 0.0, 'avg_logprob': -0.19387703641838983, 'compression_ratio': 1.5971223021582734, 'no_speech_prob': 0.10198593139648438}, {'id': 38, 'seek': 11300, 'start': 135.0, 'end': 139.0, 'text': ' Another threat is flooding, which can damage national stores of seeds.', 'tokens': [51464, 3996, 4734, 307, 24132, 11, 597, 393, 4344, 4048, 9512, 295, 9203, 13, 51664], 'temperature': 0.0, 'avg_logprob': -0.19387703641838983, 'compression_ratio': 1.5971223021582734, 'no_speech_prob': 0.10198593139648438}, {'id': 39, 'seek': 11300, 'start': 139.0, 'end': 142.0, 'text': ' This happened in the Philippines.', 'tokens': [51664, 639, 2011, 294, 264, 20153, 13, 51814], 'temperature': 0.0, 'avg_logprob': -0.19387703641838983, 'compression_ratio': 1.5971223021582734, 'no_speech_prob': 0.10198593139648438}, {'id': 40, 'seek': 14200, 'start': 143.0, 'end': 145.0, 'text': ' And with industrial-scale farming,', 'tokens': [50414, 400, 365, 9987, 12, 20033, 16557, 11, 50514], 'temperature': 0.0, 'avg_logprob': -0.22211185578377016, 'compression_ratio': 1.625, 'no_speech_prob': 0.010019141249358654}, {'id': 41, 'seek': 14200, 'start': 145.0, 'end': 148.0, 'text': ' most food comes from just a dozen varieties of plants.', 'tokens': [50514, 881, 1755, 1487, 490, 445, 257, 16654, 22092, 295, 5972, 13, 50664], 'temperature': 0.0, 'avg_logprob': -0.22211185578377016, 'compression_ratio': 1.625, 'no_speech_prob': 0.010019141249358654}, {'id': 42, 'seek': 14200, 'start': 148.0, 'end': 152.0, 'text': ' So keeping different genetic types helps to guarantee supplies.', 'tokens': [50664, 407, 5145, 819, 12462, 3467, 3665, 281, 10815, 11768, 13, 50864], 'temperature': 0.0, 'avg_logprob': -0.22211185578377016, 'compression_ratio': 1.625, 'no_speech_prob': 0.010019141249358654}, {'id': 43, 'seek': 14200, 'start': 153.0, 'end': 156.0, 'text': ' It is for the survival of mankind in the future.', 'tokens': [50914, 467, 307, 337, 264, 12559, 295, 21220, 294, 264, 2027, 13, 51064], 'temperature': 0.0, 'avg_logprob': -0.22211185578377016, 'compression_ratio': 1.625, 'no_speech_prob': 0.010019141249358654}, {'id': 44, 'seek': 14200, 'start': 156.0, 'end': 160.0, 'text': ' We need the diversity, all the different kinds of plant material,', 'tokens': [51064, 492, 643, 264, 8811, 11, 439, 264, 819, 3685, 295, 3709, 2527, 11, 51264], 'temperature': 0.0, 'avg_logprob': -0.22211185578377016, 'compression_ratio': 1.625, 'no_speech_prob': 0.010019141249358654}, {'id': 45, 'seek': 14200, 'start': 160.0, 'end': 163.0, 'text': ' to get food for the future.', 'tokens': [51264, 281, 483, 1755, 337, 264, 2027, 13, 51414], 'temperature': 0.0, 'avg_logprob': -0.22211185578377016, 'compression_ratio': 1.625, 'no_speech_prob': 0.010019141249358654}, {'id': 46, 'seek': 14200, 'start': 163.0, 'end': 166.0, 'text': ' We have a lot of problems seeing us now.', 'tokens': [51414, 492, 362, 257, 688, 295, 2740, 2577, 505, 586, 13, 51564], 'temperature': 0.0, 'avg_logprob': -0.22211185578377016, 'compression_ratio': 1.625, 'no_speech_prob': 0.010019141249358654}, {'id': 47, 'seek': 14200, 'start': 166.0, 'end': 168.0, 'text': ' Climate change, environmental problems,', 'tokens': [51564, 27025, 1319, 11, 8303, 2740, 11, 51664], 'temperature': 0.0, 'avg_logprob': -0.22211185578377016, 'compression_ratio': 1.625, 'no_speech_prob': 0.010019141249358654}, {'id': 48, 'seek': 16800, 'start': 168.0, 'end': 171.0, 'text': ' and to tackle that, we need genetic variation.', 'tokens': [50364, 293, 281, 14896, 300, 11, 321, 643, 12462, 12990, 13, 50514], 'temperature': 0.0, 'avg_logprob': -0.2205673340828188, 'compression_ratio': 1.3511904761904763, 'no_speech_prob': 0.03374144062399864}, {'id': 49, 'seek': 16800, 'start': 172.0, 'end': 174.0, 'text': ' So in these remote mountains,', 'tokens': [50564, 407, 294, 613, 8607, 10233, 11, 50664], 'temperature': 0.0, 'avg_logprob': -0.2205673340828188, 'compression_ratio': 1.3511904761904763, 'no_speech_prob': 0.03374144062399864}, {'id': 50, 'seek': 16800, 'start': 174.0, 'end': 177.0, 'text': ' this place is meant to be a safeguard against apocalypse,', 'tokens': [50664, 341, 1081, 307, 4140, 281, 312, 257, 40153, 1970, 42600, 11, 50814], 'temperature': 0.0, 'avg_logprob': -0.2205673340828188, 'compression_ratio': 1.3511904761904763, 'no_speech_prob': 0.03374144062399864}, {'id': 51, 'seek': 16800, 'start': 177.0, 'end': 180.0, 'text': ' an insurance policy for a warming world.', 'tokens': [50814, 364, 7214, 3897, 337, 257, 17983, 1002, 13, 50964], 'temperature': 0.0, 'avg_logprob': -0.2205673340828188, 'compression_ratio': 1.3511904761904763, 'no_speech_prob': 0.03374144062399864}, {'id': 52, 'seek': 16800, 'start': 180.0, 'end': 185.0, 'text': ' David Shukman, BBC News, in Svalbard in the Arctic.', 'tokens': [50964, 4389, 1160, 2034, 1601, 11, 22669, 7987, 11, 294, 318, 3337, 29984, 294, 264, 27241, 13, 51214], 'temperature': 0.0, 'avg_logprob': -0.2205673340828188, 'compression_ratio': 1.3511904761904763, 'no_speech_prob': 0.03374144062399864}], 'language': 'en'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yt-dlp -f 'ba' https://www.youtube.com/watch?v=O67m2k70JLA -o '%(title)s.%(ext)s'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhrcfXME_VUF",
        "outputId": "37ed3f87-ec64-4ff3-814d-6d8450f07ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=O67m2k70JLA\n",
            "[youtube] O67m2k70JLA: Downloading webpage\n",
            "[youtube] O67m2k70JLA: Downloading ios player API JSON\n",
            "[youtube] O67m2k70JLA: Downloading android player API JSON\n",
            "[youtube] O67m2k70JLA: Downloading m3u8 information\n",
            "[info] O67m2k70JLA: Downloading 1 format(s): 251\n",
            "[download] Destination: The Ancient University of Nalanda ｜ It Happens Only in India ｜ National Geographic.webm\n",
            "\u001b[K[download] 100% of    3.44MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m20.04MiB/s\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_test = model.transcribe(\"/content/The_Ancient_University_of_Nalanda.webm\")\n",
        "print(hindi_test[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNiYkgEe95BG",
        "outputId": "598bf799-edb8-4714-c25b-daa7da014017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " आज कोई बैठरीन युनिवर्सिटीज की बात करें, तो दिमाग में आते हैं ओक्सवर्ड, येएँ और केम्बरिज जैसे नाव। लेकिन एक कमाल की बात बता, करीब 1500 साल पहले कहानी कुछ और ही थी। तब भारत में थी एक एसी युनिवर्सिटी जो आज दुनिया की सबसे पुरानी युनिवर्सिटीज में से एक है नालंदा। नालंदा शब तीन शब्दों से बना है, न पलस आलम पलस दा, इसका मतलब है एक ऐसा उपहार जिसकी कोई सीमा नहीं हो। वाकई, जैसा नाम वैसा काव। इस युनिवर्सिटी में ज्यान का भंडार था, यहां सिर्फ धार्मेक ग्रंत ही नहीं, बलकी लिटरिचर, फियोलोजी, लोजिक, मेडिसिन और फिलोसिफी जैसे कई सबजेक्स पढ़ाय जाते थे। और तो और, हर सबजेक्स का भरपूर ज्यान देनी के लिए बनाई गई थी नौ मंसला लाइप्रिवर्व। जिसमें लाक, दू लाक नहीं, नबे लाक से भी ज्यादा बुक्स और मैनिस्क्रिप्स थी। ये युनिबर्सिटी साथ सो साल तक दुनिया को सीख देदी रही। लेकिन नालंडा ने कई बार हमलों की मार को भी जेला। आखिरकार, बारहवी शतावधी में, एक हमले में बखतियार खिल जी ने नालंडा को जला दिया। और माना जाता है कि इसकी लाइबरियी तीन महीने तक जलती रही। और ग्यान का वो ख़जाना जो समभाल कर रखा गया था, दुए में खो गया। उस वख जब नालंडा कई बुलंदियों को छू रही थी, वो एक महा विहार यानि विशाल बौध्ध मढ़ का हिस्सा थी। जिसके खंड़हर आज भी 57 एकर में भेली इस साइट में नजर आते हैं। कुछ रिकाउट के मताबिएं, नालंडा दरसल आम के बघीचे पर बनी थी, जिसी 500 व्यापारियों ने गौतम बुद्ध को तुफे में दिया। पर आक्यॉलजी के हिसाब से, नालंडा को 5th century BC में गुप्तकाल में बनाया गया था। अलग-अलग दौर में, यहाँ कई महान टीचर्स ने पढ़ाया, जैसे नाकरजुना, बुद्धपालिता, पूर्टवश्वारी ने पढ़ाया। इसने बहुत बहुत बहुत विखार की अधित्सवारी ने आपको सुन रहा है। इस जगर का और दिवेलप्मेंट हुआ और ये एक विहार से साथमी शतापती तक महाविहार बन चुका जीए। अलग-अलग दोर में यहाँ कई महान टीचर्स ने पढ़ाया, जैसे नागरजुना, बुद्धपालिता, शांत रक्षिता और आर्यदेव. आप पर पूरे विश्व से, चीन से, मियानमार से, श्रीलंका से, अनेक प्रांतों के लोग इस जगापर शिक्षा गरहन के लिए आया करते थे, इस महाविहार में. लेकिन इस सब के बावजूद, नालंदा वक्त की मार को नहीं जहिल पाई. नालंदा जैसे उनिवर्सिटी इस बात का सुबूत है, कि भारत का ज्यान सदियों से दुनिया को रोशन कर रहा है. हम उमीद करते हैं. वक्त एक बार फिर करवट लेगा, और बहतरीन एडीकेशन की तलाश करता हर इंसान कहेगा, इद हापन्स ओन्ली इन इंडिया.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_test[\"text\"]"
      ],
      "metadata": {
        "id": "BO5twGhp_n1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Just change the following prompt to generate Chapters for YouTube videos"
      ],
      "metadata": {
        "id": "qXkrf92PHY-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "model_path = \"openlm-research/open_llama_7b\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
        ")\n",
        "\n",
        "pipeline = pipeline(\n",
        "    \"text-generation\", #task\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=1000,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "a4f7a65be8154e8aab2df1d36b99bf97",
            "fdda3f1be9204a07a58130c8a57628e2",
            "77f2d9870952441b9cec2bf094d3ea9d",
            "907ef2380323488f9d3f22fe3a20a4f4",
            "b05bd91e271b4b6aa9e3276fff389837",
            "0ec1682c28f041d39badc759fd3cfa5b",
            "1ee100deb6b34f22a0e404fd272262d0",
            "c97253629708410897f21bb6e890b0bc",
            "9a6585d8fda34b5fb6e17bb64eb88b8a",
            "c2f6a6b282b24b1281aef5b7dc8e9829",
            "e260cdec6dd04526b641940e585e1590"
          ]
        },
        "id": "SNu9GNwlCr1R",
        "outputId": "4419d801-6d84-48b9-8665-bbfa99ee75e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.utils.modeling:The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4f7a65be8154e8aab2df1d36b99bf97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "id": "LL7JGQ5iCzIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "template = \"\"\"\n",
        "              Write a concise summary of the following text delimited by triple backquotes.\n",
        "              Return your response in bullet points which covers the key points of the text.\n",
        "              ```{text}```\n",
        "              BULLET POINT SUMMARY:\n",
        "           \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "text = \"\"\" As part of Meta’s commitment to open science, today we are publicly releasing LLaMA (Large Language Model Meta AI), a state-of-the-art foundational large language model designed to help researchers advance their work in this subfield of AI. Smaller, more performant models such as LLaMA enable others in the research community who don’t have access to large amounts of infrastructure to study these models, further democratizing access in this important, fast-changing field.\n",
        "\n",
        "Training smaller foundation models like LLaMA is desirable in the large language model space because it requires far less computing power and resources to test new approaches, validate others’ work, and explore new use cases. Foundation models train on a large set of unlabeled data, which makes them ideal for fine-tuning for a variety of tasks. We are making LLaMA available at several sizes (7B, 13B, 33B, and 65B parameters) and also sharing a LLaMA model card that details how we built the model in keeping with our approach to Responsible AI practices.\n",
        "\n",
        "Over the last year, large language models — natural language processing (NLP) systems with billions of parameters — have shown new capabilities to generate creative text, solve mathematical theorems, predict protein structures, answer reading comprehension questions, and more. They are one of the clearest cases of the substantial potential benefits AI can offer at scale to billions of people.\n",
        "\n",
        "Even with all the recent advancements in large language models, full research access to them remains limited because of the resources that are required to train and run such large models. This restricted access has limited researchers’ ability to understand how and why these large language models work, hindering progress on efforts to improve their robustness and mitigate known issues, such as bias, toxicity, and the potential for generating misinformation.\n",
        "\n",
        "Smaller models trained on more tokens — which are pieces of words — are easier to retrain and fine-tune for specific potential product use cases. We trained LLaMA 65B and LLaMA 33B on 1.4 trillion tokens. Our smallest model, LLaMA 7B, is trained on one trillion tokens.\n",
        "\n",
        "Like other large language models, LLaMA works by taking a sequence of words as an input and predicts a next word to recursively generate text. To train our model, we chose text from the 20 languages with the most speakers, focusing on those with Latin and Cyrillic alphabets.\n",
        "\n",
        "There is still more research that needs to be done to address the risks of bias, toxic comments, and hallucinations in large language models. Like other models, LLaMA shares these challenges. As a foundation model, LLaMA is designed to be versatile and can be applied to many different use cases, versus a fine-tuned model that is designed for a specific task. By sharing the code for LLaMA, other researchers can more easily test new approaches to limiting or eliminating these problems in large language models. We also provide in the paper a set of evaluations on benchmarks evaluating model biases and toxicity to show the model’s limitations and to support further research in this crucial area.\n",
        "\n",
        "To maintain integrity and prevent misuse, we are releasing our model under a noncommercial license focused on research use cases. Access to the model will be granted on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world. People interested in applying for access can find the link to the application in our research paper.\n",
        "\n",
        "We believe that the entire AI community — academic researchers, civil society, policymakers, and industry — must work together to develop clear guidelines around responsible AI in general and responsible large language models in particular. We look forward to seeing what the community can learn — and eventually build — using LLaMA.\n",
        "\"\"\"\n",
        "\n",
        "print(llm_chain.run(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN_t-Td3C0Aq",
        "outputId": "bf83886f-7411-420e-90cf-8cc1b98ad349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. LLaMA is a foundational large language model designed to help researchers advance their work in the AI subfield of natural language processing, which has a wide range of potential applications.\n",
            "\n",
            "2. LLaMA is designed to be versatile, meaning it can be applied to many different use cases.\n",
            "\n",
            "3. By sharing the code for LLaMA, other researchers can more easily test new approaches to limiting or eliminating biases or toxicity in large language models.\n",
            "\n",
            "4. In the future, researchers will be able to study the limits of LLaMA and use that information to develop new techniques that prevent or mitigate the problems\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"\"\"\n",
        "Tesla, Inc. (/ˈtɛslə/ TESS-lə or /ˈtɛzlə/ TEZ-lə[a]) is an American multinational automotive and clean energy company headquartered in Austin, Texas. Tesla designs and manufactures electric vehicles (cars and trucks), stationary battery energy storage devices from home to grid-scale, solar panels and solar roof tiles, and related products and services.\n",
        "\n",
        "Tesla is one of the world's most valuable companies and, as of 2023, was the world's most valuable automaker. In 2022, the company led the battery electric vehicle market, with 18% share.\n",
        "\n",
        "Its subsidiary Tesla Energy develops and is a major installer of photovoltaic systems in the United States. Tesla Energy is one of the largest global suppliers of battery energy storage systems, with 6.5 gigawatt-hours (GWh) installed in 2022.\n",
        "\n",
        "Tesla was incorporated in July 2003 by Martin Eberhard and Marc Tarpenning as Tesla Motors. The company's name is a tribute to inventor and electrical engineer Nikola Tesla. In February 2004, via a $6.5 million investment, Elon Musk became the company's largest shareholder. He became CEO in 2008. Tesla's announced mission is to help expedite the move to sustainable transport and energy, obtained through electric vehicles and solar power.\n",
        "\n",
        "Tesla began production of its first car model, the Roadster sports car, in 2008. This was followed by the Model S sedan in 2012, the Model X SUV in 2015, the Model 3 sedan in 2017, the Model Y crossover in 2020, and the Tesla Semi truck in 2022. The company plans production of the Cybertruck light-duty pickup truck in 2023.[8] The Model 3 is the all-time bestselling plug-in electric car worldwide, and in June 2021 became the first electric car to sell 1 million units globally.[9] Tesla's 2022 deliveries were around 1.31 million vehicles, a 40% increase over the previous year,[10][11] and cumulative sales totaled 4 million cars as of April 2023.[12] In October 2021, Tesla's market capitalization temporarily reached $1 trillion, the sixth company to do so in U.S. history.\n",
        "\n",
        "Tesla has been the subject of lawsuits, government scrutiny, and journalistic criticism, stemming from allegations of whistleblower retaliation, worker rights violations, product defects, and Musk's many controversial statements.\n",
        "\"\"\"\n",
        "\n",
        "print(llm_chain.run(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMht9XnjGAou",
        "outputId": "1405cc66-1f6f-414e-fab1-23d8b92e8b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1. The company was founded in the early 2000's to help reduce the amount of pollution in the US, by creating clean and efficient electric vehicles.\n",
            " 2. The company's first model, the Roadster car was the world's first mass market fully electric vehicle which went into production in 2008.\n",
            " 3. The company's first SUV model, the Model X, went into production in 2012.\n",
            " 4. In 2015 the Model S sedan was introduced.\n",
            " 5. In 2017, the Model 3 sedan was introduced.\n",
            " 6. In 2020, the Model Y SUV and Tesla Semi truck models were introduced.\n",
            " 7. The company's market capitalization reached $1 trillion for the first time, which made it the sixth US company to do so.\n",
            "\n",
            "#### **Solution**\n",
            "\n",
            "Tesla, Inc. is a company that develops and manufactures electric vehicles, solar panels, and battery energy storage systems. Tesla has been the most valuable car company in the world, and it is also one of the most profitable automotive companies in the world. The company has received a great deal of media attention for its success. \n",
            "The company was founded in the early 2000's to help reduce the amount of pollution in the US, by creating clean and efficient electric vehicles. The company's first model, the Roadster sports car, went into production in 2008. The company's first SUV model, the Model X, went into\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 =\"\"\"\n",
        "Apple Inc. is an American multinational technology company headquartered in Cupertino, California. Apple is the world's largest technology company by revenue, with US$394.3 billion in 2022 revenue.[6] As of March 2023, Apple is the world's biggest company by market capitalization.[7] As of June 2022, Apple is the fourth-largest personal computer vendor by unit sales and the second-largest mobile phone manufacturer in the world. It is often considered as one of the Big Five American information technology companies, alongside Alphabet (parent company of Google), Amazon, Meta Platforms, and Microsoft.\n",
        "\n",
        "Apple was founded as Apple Computer Company on April 1, 1976, by Steve Wozniak, Steve Jobs (1955–2011) and Ronald Wayne to develop and sell Wozniak's Apple I personal computer. It was incorporated by Jobs and Wozniak as Apple Computer, Inc. in 1977. The company's second computer, the Apple II, became a best seller and one of the first mass-produced microcomputers. Apple went public in 1980 to instant financial success. The company developed computers featuring innovative graphical user interfaces, including the 1984 original Macintosh, announced that year in a critically acclaimed advertisement called \"1984\". By 1985, the high cost of its products, and power struggles between executives, caused problems. Wozniak stepped back from Apple and pursued other ventures, while Jobs resigned and founded NeXT, taking some Apple employees with him.\n",
        "\n",
        "As the market for personal computers expanded and evolved throughout the 1990s, Apple lost considerable market share to the lower-priced duopoly of the Microsoft Windows operating system on Intel-powered PC clones (also known as \"Wintel\"). In 1997, weeks away from bankruptcy, the company bought NeXT to resolve Apple's unsuccessful operating system strategy and entice Jobs back to the company. Over the next decade, Jobs guided Apple back to profitability through a number of tactics including introducing the iMac, iPod, iPhone and iPad to critical acclaim, launching the \"Think different\" campaign and other memorable advertising campaigns, opening the Apple Store retail chain, and acquiring numerous companies to broaden the company's product portfolio. When Jobs resigned in 2011 for health reasons, and died two months later, he was succeeded as CEO by Tim Cook.\n",
        "\n",
        "Apple became the first publicly traded U.S. company to be valued at over $1 trillion in August 2018, then at $2 trillion in August 2020, and at $3 trillion in January 2022. As of April 2023, it was valued at around $2.6 trillion. The company receives criticism regarding the labor practices of its contractors, its environmental practices, and its business ethics, including anti-competitive practices and materials sourcing. Nevertheless, the company has a large following and enjoys a high level of brand loyalty. It has also been consistently ranked as one of the world's most valuable brands.\n",
        "\"\"\"\n",
        "print(llm_chain.run(text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GbGQyZJLn2o",
        "outputId": "39b0bb48-f842-4238-d7ea-fd1988ca6d20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Apple Inc. is an American multinational technology company and the second-largest company by market capitalization in the world. Apple is the first company valued at more than $1 trillion. The company is the third-most valuable company by market capitalization. The company was founded on April 1, 1976, by Steve Wozniak, Steve Jobs (1955–2011), and Ron Wayne. The original company name was Apple Computer Company, with the name changed to simply \"Apple\" in 1977. The company's second computer, the Apple II was a commercial success. The company went public in 1980 to instant financial success, with Jobs as the CEO and chief product officer. Jobs resigned from Apple in 1985 after losing a power struggle with his then-colleague, John Sculley. Wozniak, who had already been out of the company for two years, remained as an adviser. In 1997, weeks away from bankruptcy, the company bought NeXT to resolve Apple's unsuccessful operating system strategy and entice Jobs back to the company. Over the next decade, Jobs guided Apple back to profitability through a number of tactics including introducing the iMac, iPod, iPhone, and iPad to\n"
          ]
        }
      ]
    }
  ]
}